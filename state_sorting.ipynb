{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f2bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import xarray as xr\n",
    "import numpy as np \n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# magic to autoreload modules called\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f99f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will build the path for the sessions as 'default_directory'//'experiment_directory[n]'//(specific selected session)\n",
    "default_directory = \"D://EinsteinMed Dropbox//Gabriel Baltazar//\" # root directory \n",
    "experiment_directory = [\"Chat-Cre X ai40 (Revised)\"] # [\"Chat-Cre X ai40 (Revised)\", \"VIP-Cre x Ai40 (Revised)\"] # folders that are in the root directory and store specific recording folders\n",
    "\n",
    "# metadata file\n",
    "metadata_file = 'session_metadata.json'\n",
    "criteria_dict = {\n",
    "    \"include_in_analysis\":  True,\n",
    "    \"visual_stimulation_type\": \"Drifting Grating\" # including only drifting grating bcs natural stimuli recordings didn't have spontaneous block\n",
    "}\n",
    "\n",
    "valid_paths_list = div.get_recordings_path(default_directory, experiment_directory, criteria_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7274a81",
   "metadata": {},
   "source": [
    "### Checking dropped frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87280b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(proc[\"motion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ab42d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adc_xarray = \"adc_channels.zarr\"\n",
    "\n",
    "# loop for recordings\n",
    "for session in valid_paths_list:\n",
    "\n",
    "    # loading the xarrays\n",
    "    session_processed = os.path.join(session, \"processed\")\n",
    "    adc = xr.open_zarr(os.path.join(session_processed, adc_xarray), consolidated=True)\n",
    "    camera_strobe = adc[\"camera_strobe\"].values\n",
    "\n",
    "    # getting the facemap processed file\n",
    "    npy_files = glob.glob(os.path.join(session, '*proc.npy'))\n",
    "    video_data_file_add = npy_files[0] # selecting the first .npy file by alphabetical order\n",
    "    proc = np.load(video_data_file_add, allow_pickle=True).item()\n",
    "    face_motion = proc[\"motion\"][1]\n",
    "\n",
    "    # detecting the strobe transitions\n",
    "    strobe_onset, _, _, _ = intanGab.detect_transitions(\n",
    "        camera_strobe,\n",
    "        lowpass_filter=True,\n",
    "        output_plot=False\n",
    "    )\n",
    "\n",
    "    print(f\"({os.path.basename(session)}) number of frames: {len(face_motion)}\")\n",
    "    print(f\"({os.path.basename(session)}) number of strobes: {len(strobe_onset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa07b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to worry about dropped frames, all recordings seem fine in regard to that\n",
    "\n",
    "# 'read_intan_ports.detect_transitions: detected 159368 transitions, with the start trigger as down transition\n",
    "# (ehsan_221206_100358) number of frames: 159368\n",
    "# (ehsan_221206_100358) number of strobes: 159368\n",
    "# 'read_intan_ports.detect_transitions: detected 146342 transitions, with the start trigger as up transition\n",
    "# (ehsan_221206_104734) number of frames: 146341\n",
    "# (ehsan_221206_104734) number of strobes: 146342\n",
    "# 'read_intan_ports.detect_transitions: detected 192372 transitions, with the start trigger as down transition\n",
    "# (ehsan_221207_105129) number of frames: 192372\n",
    "# (ehsan_221207_105129) number of strobes: 192372\n",
    "# 'read_intan_ports.detect_transitions: detected 186462 transitions, with the start trigger as down transition\n",
    "# (ehsan_221208_095858) number of frames: 186462\n",
    "# (ehsan_221208_095858) number of strobes: 186462\n",
    "# 'read_intan_ports.detect_transitions: detected 185990 transitions, with the start trigger as up transition\n",
    "# (ehsan_221208_103742) number of frames: 185989\n",
    "# (ehsan_221208_103742) number of strobes: 185990\n",
    "# 'read_intan_ports.detect_transitions: detected 197245 transitions, with the start trigger as up transition\n",
    "# (ehsan_221209_082822) number of frames: 197244\n",
    "# (ehsan_221209_082822) number of strobes: 197245\n",
    "# 'read_intan_ports.detect_transitions: detected 160860 transitions, with the start trigger as up transition\n",
    "# (ehsan_210415_102404) number of frames: 160858\n",
    "# (ehsan_210415_102404) number of strobes: 160860\n",
    "# 'read_intan_ports.detect_transitions: detected 164406 transitions, with the start trigger as up transition\n",
    "# (ehsan_210416_124048) number of frames: 164405\n",
    "# (ehsan_210416_124048) number of strobes: 164406\n",
    "# 'read_intan_ports.detect_transitions: detected 161685 transitions, with the start trigger as up transition\n",
    "# (ehsan_210416_150531) number of frames: 161684\n",
    "# (ehsan_210416_150531) number of strobes: 161685\n",
    "# 'read_intan_ports.detect_transitions: detected 193498 transitions, with the start trigger as up transition\n",
    "# (ehsan_210422_103751) number of frames: 193497\n",
    "# (ehsan_210422_103751) number of strobes: 193498\n",
    "# 'read_intan_ports.detect_transitions: detected 213467 transitions, with the start trigger as up transition\n",
    "# (ehsan_210422_135429) number of frames: 213466\n",
    "# (ehsan_210422_135429) number of strobes: 213467\n",
    "# 'read_intan_ports.detect_transitions: detected 189261 transitions, with the start trigger as up transition\n",
    "# (ehsan_210423_100903) number of frames: 189260\n",
    "# (ehsan_210423_100903) number of strobes: 189261\n",
    "# 'read_intan_ports.detect_transitions: detected 188361 transitions, with the start trigger as up transition\n",
    "# (ehsan_210423_124453) number of frames: 188360\n",
    "# (ehsan_210423_124453) number of strobes: 188361\n",
    "# 'read_intan_ports.detect_transitions: detected 193850 transitions, with the start trigger as up transition\n",
    "# (ehsan_210506_125649) number of frames: 193849\n",
    "# (ehsan_210506_125649) number of strobes: 193850\n",
    "# 'read_intan_ports.detect_transitions: detected 136499 transitions, with the start trigger as up transition\n",
    "# (ehsan_210506_154650) number of frames: 136498\n",
    "# (ehsan_210506_154650) number of strobes: 136499\n",
    "# 'read_intan_ports.detect_transitions: detected 175482 transitions, with the start trigger as up transition\n",
    "# (ehsan_210507_102850) number of frames: 175481\n",
    "# (ehsan_210507_102850) number of strobes: 175482\n",
    "# 'read_intan_ports.detect_transitions: detected 187046 transitions, with the start trigger as up transition\n",
    "# (ehsan_210507_133101) number of frames: 187003\n",
    "# (ehsan_210507_133101) number of strobes: 187046\n",
    "# 'read_intan_ports.detect_transitions: detected 242581 transitions, with the start trigger as up transition\n",
    "# (ehsan_210617_102107) number of frames: 242580\n",
    "# (ehsan_210617_102107) number of strobes: 242581\n",
    "# 'read_intan_ports.detect_transitions: detected 187107 transitions, with the start trigger as up transition\n",
    "# (ehsan_210618_114033) number of frames: 187106\n",
    "# (ehsan_210618_114033) number of strobes: 187107"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd026a3",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57726842",
   "metadata": {},
   "source": [
    "General settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7954f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xarray file names\n",
    "vis_stim_xarray = \"optogenetics_visual_stimulation.zarr\"\n",
    "amplifier_xarray = \"amplifier_reorder_low_and_high_pass_int16.zarr\"\n",
    "state_xarray = \"state.zarr\"\n",
    "\n",
    "# temporal domain settings\n",
    "window_size = 0.5\n",
    "time_to_plot_before = -0.5\n",
    "time_to_plot_after = 2.5\n",
    "window_to_plot = abs(time_to_plot_before) + abs(time_to_plot_after)\n",
    "int16_to_mV = 0.195\n",
    "\n",
    "# classification thresholds\n",
    "camera_sample_rate = 30\n",
    "emg_perc = 80\n",
    "face_motion_perc = 70\n",
    "face_motion_time_threshold = 5\n",
    "face_motion_window = face_motion_time_threshold * camera_sample_rate\n",
    "locomotion_threshold = 0.5\n",
    "locomotion_time_threshold = 5\n",
    "locomotion_window = locomotion_time_threshold * camera_sample_rate\n",
    "classification_threshold = 0.9\n",
    "\n",
    "# states settings\n",
    "quiescence_indicator = 0\n",
    "face_motion_indicator = 1\n",
    "locomotion_indicator = 2\n",
    "quies_label = \"quiet\"\n",
    "active_label = \"wake\"\n",
    "loc_label = \"loc.\"\n",
    "disc_label = \"disc.\"\n",
    "all_labels = np.asarray([quies_label, active_label, loc_label, disc_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc9e4de",
   "metadata": {},
   "source": [
    "Figure settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bfbd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure settings\n",
    "figure_size = [14, 7]\n",
    "figure_rows = 16\n",
    "figure_columns = 4\n",
    "\n",
    "# subplots settings\n",
    "first_row = slice(0, 3)\n",
    "second_row = slice(3, 6)\n",
    "third_row = slice(6, 9)\n",
    "fourth_row = slice(9, 12)\n",
    "fifth_row = slice(12, 13)\n",
    "sixth_row = slice(13, 14)\n",
    "seventh_row = slice(14, 15)\n",
    "eigth_row = slice(15, 16)\n",
    "trial_row = slice(12, 16)\n",
    "col_left = slice(0, 1)\n",
    "col_midl = slice(1, 2)\n",
    "col_midr = slice(2, 3)\n",
    "col_right = slice(3, 4)\n",
    "col_sess = slice(0, 3)\n",
    "\n",
    "# figure colors\n",
    "replicates_color = \"#ebebeb\"\n",
    "window_color =  \"#f4c542\" #\"#808080\"\n",
    "quies_color = \"#6666ff\"#\"#bfbfbf\"\n",
    "active_color = \"#66ff66\"#\"#808080\"\n",
    "loc_color = \"#ff6666\"#\"#000000\"\n",
    "disc_color = \"#000000\"#\"#ff6666\"\n",
    "state_color = \"#000000\"#\"#1f77b4\"\n",
    "opto_off_color = \"#000000\"\n",
    "opto_on_color = \"#0cb7f4\"\n",
    "highlight_color = \"#FF0000\"\n",
    "\n",
    "save_fig = True\n",
    "folder_to_save = os.path.join(\"D://EinsteinMed Dropbox//Gabriel Baltazar//Chat-Cre VIP-Cre figs\", \"state_sorting_accurate_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe7d827",
   "metadata": {},
   "source": [
    "Plots spines settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309aed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_settings = {\n",
    "    \"left\":     {\"linewidth\": 1.2, \"tick_width\": 1.2, \"ticks\": []}, \n",
    "    \"bottom\":   {\"linewidth\": 1.2, \"tick_width\": 1.2, \"ticks\": []}, \n",
    "    \"right\":    {\"visible\": False}, \n",
    "    \"top\":      {\"visible\": False},\n",
    "    }\n",
    "\n",
    "whole_sess_settings = {\n",
    "    \"left\":     {\"visible\": False}, \n",
    "    \"bottom\":   {\"visible\": False}, \n",
    "    \"right\":    {\"visible\": False}, \n",
    "    \"top\":      {\"visible\": False},\n",
    "    }\n",
    "\n",
    "trial_settings = {\n",
    "    \"left\":     {\"linewidth\": 1.2, \"tick_width\": 1.2, \"ticks\": (\"data_lim\", 0), \"label\": \"Trial\\nCount\", \"label_pad\": -8}, \n",
    "    \"bottom\":   {\"linewidth\": 1.2, \"tick_width\": 1.2, \"tick_rotation\": 45}, \n",
    "    \"right\":    {\"visible\": False}, \n",
    "    \"top\":      {\"visible\": False},\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a5fcd1",
   "metadata": {},
   "source": [
    "State sorting and plotting figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03976775",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# loop for recordings\n",
    "for session in valid_paths_list:\n",
    "\n",
    "    # loading session metadata\n",
    "    with open(os.path.join(session, \"session_metadata.json\"), 'r') as json_file:\n",
    "        metadata = json.load(json_file)\n",
    "    # sample_rate = metadata[\"amplifier_sample_rate\"]\n",
    "    lfp_sample_rate = metadata[\"amplifier_low_pass_sample_rate\"]\n",
    "    camera_sample_rate = metadata[\"camera_sample_rate\"]\n",
    "    animal_id = metadata[\"animal_id\"]\n",
    "    experiment_id = metadata[\"experiment_id\"]\n",
    "    group = metadata[\"group\"]\n",
    "\n",
    "    # loading the xarrays\n",
    "    session_processed = os.path.join(session, \"processed\")\n",
    "    vis_stim_opto = xr.open_zarr(os.path.join(session_processed, vis_stim_xarray), consolidated=True)\n",
    "    state = xr.open_zarr(os.path.join(session_processed, state_xarray), consolidated=True)\n",
    "    amplifier = xr.open_zarr(os.path.join(session_processed, amplifier_xarray), consolidated=True)\n",
    "    \n",
    "    # processing the lfp \n",
    "    lfp_mat_chs = scipy.io.loadmat(os.path.join(session, \"amplifierReorder\", \"amplifierReorder.SleepScoreLFP.LFP.mat\"))\n",
    "    lfp_mat_emg = scipy.io.loadmat(os.path.join(session, \"amplifierReorder\", \"amplifierReorder.EMGFromLFP.LFP.mat\"))\n",
    "    th_ch = lfp_mat_chs[\"SleepScoreLFP\"][\"THchanID\"].item()[0][0]\n",
    "    sw_ch = lfp_mat_chs[\"SleepScoreLFP\"][\"SWchanID\"].item()[0][0]\n",
    "    ts_emg = lfp_mat_emg[\"EMGFromLFP\"][\"timestamps\"][0][0]\n",
    "    data_emg = lfp_mat_emg[\"EMGFromLFP\"][\"data\"][0][0]\n",
    "    lfp = amplifier[\"low_pass\"]\n",
    "    lfp_timepoint = amplifier.coords[\"time_in_sec_low_pass\"].values\n",
    "    slow_waves = filtsGab.butter_bandpass_filter(data=lfp.sel(channel=sw_ch).values, cutoff=[0.5, 4], fs=lfp_sample_rate, order=2) * int16_to_mV\n",
    "    theta_waves = filtsGab.butter_bandpass_filter(data=lfp.sel(channel=th_ch).values, cutoff=[6, 10], fs=lfp_sample_rate, order=2) * int16_to_mV\n",
    "\n",
    "    # loading state data\n",
    "    strobe_in_seconds = state.coords[\"strobe_in_seconds\"].values\n",
    "    face_motion = state[\"face_motion_normalized\"].values\n",
    "    treadmill = state[\"treadmill_analog\"].values\n",
    "    scored_state = np.zeros_like(strobe_in_seconds, dtype=int)\n",
    "\n",
    "    # scoring face motion epochs\n",
    "    face_motion_threshold = np.percentile(face_motion, face_motion_perc)\n",
    "    face_motion_inds = np.where(face_motion > face_motion_threshold)[0]\n",
    "    i = 0\n",
    "    while i < len(face_motion_inds): # looping over face motion peaks\n",
    "        \n",
    "        onset = face_motion_inds[i]\n",
    "\n",
    "        # Look ahead until the first peak farther than the window\n",
    "        j = i + 1\n",
    "        while j < len(face_motion_inds) and (face_motion_inds[j] - face_motion_inds[i]) < face_motion_window:\n",
    "            i = j\n",
    "            j += 1\n",
    "\n",
    "        # The offset is the last close peak + the window\n",
    "        offset = face_motion_inds[j-1] + face_motion_window\n",
    "        scored_state[onset:offset] = face_motion_indicator\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # scoring locomotion epochs according to treadmill and emg data\n",
    "    treadmill_inds = np.where(abs(treadmill) > locomotion_threshold)[0]\n",
    "    emg_threshold = np.percentile(data_emg, emg_perc)\n",
    "    emg_inds = np.where(data_emg > emg_threshold)[0]\n",
    "    emg_inds_converted = np.zeros(len(emg_inds))\n",
    "    for i, emg_ind in enumerate(emg_inds): # converting emg indexes\n",
    "        emg_inds_converted[i] = np.argmin(abs(strobe_in_seconds - ts_emg[emg_ind]))\n",
    "    locomotion_inds = np.union1d(treadmill_inds, emg_inds_converted).astype(int)\n",
    "\n",
    "    while i < len(locomotion_inds): # looping over face motion peaks\n",
    "        \n",
    "        onset = locomotion_inds[i]\n",
    "\n",
    "        # Look ahead until the first peak farther than the window\n",
    "        j = i + 1\n",
    "        while j < len(locomotion_inds) and (locomotion_inds[j] - locomotion_inds[i]) < locomotion_window:\n",
    "            i = j\n",
    "            j += 1\n",
    "\n",
    "        # The offset is the last close peak \n",
    "        offset = locomotion_inds[j-1]\n",
    "        scored_state[onset:offset] = locomotion_indicator\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # getting visual stimulation trials onset timestamps\n",
    "    vis_stim_mask = vis_stim_opto.coords[\"visual_stimulation\"].astype(bool)\n",
    "    stim_onset = vis_stim_opto[\"visual_stimulation_timestamp\"].sel(event_type=\"onset\").where(vis_stim_mask, drop=True).values\n",
    "    stim_offset = stim_onset + window_size\n",
    "\n",
    "    sample_no_lfp = int(lfp_sample_rate * window_to_plot)\n",
    "    sample_no_state = int(camera_sample_rate * window_to_plot)\n",
    "    trial_class = np.empty(len(stim_onset), dtype='object')\n",
    "    th_class = np.zeros([len(stim_onset), sample_no_lfp], dtype=float)\n",
    "    sw_class = np.zeros([len(stim_onset), sample_no_lfp], dtype=float)\n",
    "    tr_class = np.zeros([len(stim_onset), sample_no_state], dtype=float)\n",
    "    fm_class = np.zeros([len(stim_onset), sample_no_state], dtype=float)\n",
    "\n",
    "    # loop to state-score the trials\n",
    "    for i, (onset, offset) in enumerate(zip(stim_onset, stim_offset)):\n",
    "\n",
    "        # getting the scored-frames for the current trial\n",
    "        trial_state = scored_state[(strobe_in_seconds >= onset) & (strobe_in_seconds <= offset)]\n",
    "\n",
    "        # counting how many frames were scored in each state\n",
    "        unique_states, count_states = np.unique(trial_state, return_counts=True)\n",
    "        if np.max(count_states / len(trial_state)) >= classification_threshold: # case: homogeneous trial, at least 90% of frames with same score\n",
    "            if unique_states[np.argmax(count_states)] == quiescence_indicator: # case: quiescent trial\n",
    "                st = quies_label\n",
    "            elif unique_states[np.argmax(count_states)] == face_motion_indicator: # case: active trial\n",
    "                st = active_label\n",
    "            elif unique_states[np.argmax(count_states)] == locomotion_indicator: # case: locomotion trial\n",
    "                st = loc_label\n",
    "            trial_class[i] = st\n",
    "        else: # case: inhomogeneous trial, less than 90% of frames with same score\n",
    "            trial_class[i] = disc_label\n",
    "        \n",
    "        # getting state measures for the current trial\n",
    "        lfp_first_ind = np.argmin(abs(lfp_timepoint - (onset + time_to_plot_before)))\n",
    "        state_first_ind = np.argmin(abs(strobe_in_seconds - (onset + time_to_plot_before)))\n",
    "        th_class[i] = theta_waves[lfp_first_ind : lfp_first_ind + sample_no_lfp]\n",
    "        sw_class[i] = slow_waves[lfp_first_ind : lfp_first_ind + sample_no_lfp]\n",
    "        tr_class[i] = treadmill[state_first_ind : state_first_ind + sample_no_state]\n",
    "        fm_class[i] = face_motion[state_first_ind : state_first_ind + sample_no_state]\n",
    "\n",
    "    # getting the axis objects for all individual subplots\n",
    "    fig = plt.figure(figsize=figure_size, constrained_layout=True)\n",
    "    grid = fig.add_gridspec(figure_rows, figure_columns)\n",
    "    ax_quiet_th = fig.add_subplot(grid[first_row, col_left])\n",
    "    ax_quiet_sw = fig.add_subplot(grid[second_row, col_left])\n",
    "    ax_quiet_tr = fig.add_subplot(grid[third_row, col_left])\n",
    "    ax_quiet_fm = fig.add_subplot(grid[fourth_row, col_left])\n",
    "    ax_motion_th = fig.add_subplot(grid[first_row, col_midl])\n",
    "    ax_motion_sw = fig.add_subplot(grid[second_row, col_midl])\n",
    "    ax_motion_tr = fig.add_subplot(grid[third_row, col_midl])\n",
    "    ax_motion_fm = fig.add_subplot(grid[fourth_row, col_midl])\n",
    "    ax_loc_th = fig.add_subplot(grid[first_row, col_midr])\n",
    "    ax_loc_sw = fig.add_subplot(grid[second_row, col_midr])\n",
    "    ax_loc_tr = fig.add_subplot(grid[third_row, col_midr])\n",
    "    ax_loc_fm = fig.add_subplot(grid[fourth_row, col_midr])\n",
    "    ax_disc_th = fig.add_subplot(grid[first_row, col_right])\n",
    "    ax_disc_sw = fig.add_subplot(grid[second_row, col_right])\n",
    "    ax_disc_tr = fig.add_subplot(grid[third_row, col_right])\n",
    "    ax_disc_fm = fig.add_subplot(grid[fourth_row, col_right])\n",
    "    ax_emg_sess = fig.add_subplot(grid[fifth_row, col_sess])\n",
    "    ax_tread_sess = fig.add_subplot(grid[sixth_row, col_sess])\n",
    "    ax_facem_sess = fig.add_subplot(grid[seventh_row, col_sess])\n",
    "    ax_state_sess = fig.add_subplot(grid[eigth_row, col_sess])\n",
    "    ax_trial_count = fig.add_subplot(grid[trial_row, col_right])\n",
    "\n",
    "    theta_ylims = []; delta_ylims = []; tread_ylims = []; motion_ylims = []\n",
    "\n",
    "    # looping over groups of axis to plot the theta, delta, treadmill and face motion data for each trial type\n",
    "    for ax_theta, ax_delta, ax_tread, ax_motion, trial_type, color in zip(\n",
    "        [ax_quiet_th,  ax_motion_th,  ax_loc_th,  ax_disc_th],\n",
    "        [ax_quiet_sw,  ax_motion_sw,  ax_loc_sw,  ax_disc_sw],\n",
    "        [ax_quiet_tr,  ax_motion_tr,  ax_loc_tr,  ax_disc_tr],\n",
    "        [ax_quiet_fm,  ax_motion_fm,  ax_loc_fm,  ax_disc_fm],\n",
    "        [quies_label,  active_label,  loc_label,  disc_label],\n",
    "        [quies_color,  active_color,  loc_color,  disc_color]\n",
    "        ):\n",
    "        \n",
    "        # getting trials that match the current trial type\n",
    "        selected_trials = np.where(trial_class == trial_type)[0]\n",
    "        ax_theta.set_title(f\"{trial_type}: {len(selected_trials)} trials\", fontsize=10)\n",
    "\n",
    "        # checking if there is at least one trial classified in the current trial type\n",
    "        if len(selected_trials) > 0: # case: at least one trial for the current trial type\n",
    "\n",
    "            # plotting the filetered lfp measures\n",
    "            time_lfp_to_plot = (np.arange(sample_no_lfp) / lfp_sample_rate) + time_to_plot_before\n",
    "            div.plot_with_sem(axis_to_plot = ax_theta,\n",
    "                              sem = \"compute\",\n",
    "                              signal_x = time_lfp_to_plot,\n",
    "                              signal_y = th_class[selected_trials],\n",
    "                              color = color, line_width = 2,\n",
    "                              zorder=1)\n",
    "            div.plot_with_sem(axis_to_plot = ax_delta,\n",
    "                              sem = \"compute\",\n",
    "                              signal_x = time_lfp_to_plot,\n",
    "                              signal_y = sw_class[selected_trials],\n",
    "                              color = color, line_width = 2,\n",
    "                              zorder=1)\n",
    "\n",
    "            # plotting the state measures\n",
    "            time_state_to_plot = (np.arange(sample_no_state) / camera_sample_rate) + time_to_plot_before\n",
    "            ax_tread.plot(time_state_to_plot, abs(tr_class[selected_trials].T), linewidth=0.5, color = replicates_color, zorder=0)\n",
    "            ax_tread.plot(time_state_to_plot, abs(np.mean(tr_class[selected_trials], axis=0)), linewidth=2, color = color, zorder=1)\n",
    "            ax_tread.plot(time_state_to_plot, np.ones(len(time_state_to_plot)) * locomotion_threshold, color = \"#808080\", zorder = 2, linestyle = \"--\")\n",
    "            ax_motion.plot(time_state_to_plot, fm_class[selected_trials].T, linewidth=0.5, color = replicates_color, zorder=0)\n",
    "            ax_motion.plot(time_state_to_plot, np.mean(fm_class[selected_trials], axis=0), linewidth=2, color = color, zorder=1)\n",
    "            ax_motion.plot(time_state_to_plot, np.ones(len(time_state_to_plot)) * face_motion_threshold, color = \"#808080\", zorder = 2, linestyle = \"--\")\n",
    "\n",
    "            # highlighting the window considered for the visual response\n",
    "            for axis in [ax_theta, ax_delta, ax_tread, ax_motion]:\n",
    "                div.highlight_event(axis_to_plot = axis,\n",
    "                                    highlight_type = \"shaded_area\",\n",
    "                                    event_onset = 0, event_offset = window_size,\n",
    "                                    event_color = window_color, zorder=3)\n",
    "                \n",
    "            # cumulating ylims to allow for inter-state comparisons (all plots with same scale)\n",
    "            theta_ylims.append(ax_theta.get_ylim())\n",
    "            delta_ylims.append(ax_delta.get_ylim())\n",
    "            tread_ylims.append(ax_tread.get_ylim())\n",
    "            motion_ylims.append(ax_motion.get_ylim())\n",
    "\n",
    "    # whole session plots\n",
    "    ax_emg_sess.plot(ts_emg[3:-3], abs(data_emg[3:-3]), linewidth = 0.5, color = state_color)\n",
    "    ax_emg_sess.plot(ts_emg[3:-3], np.ones(len(data_emg[3:-3])) * emg_threshold, linewidth = 0.75, color= highlight_color, linestyle= \"--\")\n",
    "    ax_tread_sess.plot(strobe_in_seconds, abs(treadmill), linewidth = 0.5, color = state_color)\n",
    "    ax_tread_sess.plot(strobe_in_seconds, np.ones(len(treadmill)) * locomotion_threshold, linewidth = 0.75, color=highlight_color, linestyle='--')\n",
    "    ax_facem_sess.plot(strobe_in_seconds, face_motion, linewidth=0.5, color = state_color)\n",
    "    ax_facem_sess.plot(strobe_in_seconds, np.ones(len(face_motion)) * face_motion_threshold, linewidth = 0.75, color=highlight_color, linestyle='--')\n",
    "\n",
    "    stateGab.plot_states(axis_to_plot=ax_state_sess, state_score=scored_state, frames_in_sec=strobe_in_seconds, input_format=\"integer\",\n",
    "                         colors = [quies_color, active_color, loc_color, disc_color])\n",
    "\n",
    "    # getting trial-wise info\n",
    "    opto_on_mask = vis_stim_opto.coords[\"opto_stimulation\"].where(vis_stim_mask).dropna(dim=\"trial\").astype(bool).values\n",
    "    trial_direcs = vis_stim_opto.coords[\"stimulus_orientation\"].where(vis_stim_mask).dropna(dim=\"trial\").values\n",
    "\n",
    "    # counting number of state-scored trials for opto-on and opto-off conditions\n",
    "    unique_directions = np.unique(trial_direcs)\n",
    "    opto_on_scored_trials = np.zeros([len(all_labels), len(unique_directions)])\n",
    "    opto_off_scored_trials = np.zeros([len(all_labels), len(unique_directions)])\n",
    "\n",
    "    # all_labels = np.unique(trial_class)\n",
    "    unique_directions = np.sort(np.unique(trial_direcs))\n",
    "\n",
    "    # Expanding dimensions for broadcasting\n",
    "    labels_exp = trial_class[:, None]       # shape (n_trials, 1)\n",
    "    direcs_exp = trial_direcs[:, None]      # shape (n_trials, 1)\n",
    "    opto_exp = opto_on_mask[:, None, None]        # shape (n_trials, 1, 1)\n",
    "\n",
    "    # Compare with each label/direction\n",
    "    label_matches = labels_exp == all_labels[None, :]        # shape (n_trials, n_labels)\n",
    "    direc_matches = direcs_exp == unique_directions[None, :] # shape (n_trials, n_directions)\n",
    "\n",
    "    # Count opto-on and opto-off trials\n",
    "    opto_on_counts = np.sum(opto_exp & label_matches[:, :, None] & direc_matches[:, None, :], axis=0)\n",
    "    opto_off_counts = np.sum(~opto_exp & label_matches[:, :, None] & direc_matches[:, None, :], axis=0)\n",
    "\n",
    "    # reshaping the data to have it grouped for the scatter plot\n",
    "    grouped = np.stack([opto_off_counts, opto_on_counts], axis=0)\n",
    "    grouped = grouped.transpose(0, 2, 1)\n",
    "    div.categorical_scatter(\n",
    "        axis = ax_trial_count,\n",
    "        grouped_data = grouped,\n",
    "        conditions_labels = all_labels,\n",
    "        group_labels= [\"Opto-OFF\", \"Opto-ON\"],\n",
    "        replicates_color = replicates_color,\n",
    "        averages_color = [opto_off_color, opto_on_color],\n",
    "        color_scheme_avg = \"group\",\n",
    "        group_space = 0.4\n",
    "    )\n",
    "    \n",
    "    # adding legend to the scatter plot\n",
    "    handles, labels = ax_trial_count.get_legend_handles_labels()\n",
    "    ax_trial_count.legend(handles[:2], labels[:2], fontsize=7, frameon=False)\n",
    "\n",
    "    # getting general ylim for all plots\n",
    "    theta_ylim = [np.min(theta_ylims), np.max(theta_ylims)]\n",
    "    delta_ylim = [np.min(delta_ylims), np.max(delta_ylims)]\n",
    "    tread_ylim = [np.min(tread_ylims), np.max(tread_ylims)]\n",
    "    motion_ylim = [np.min(motion_ylims), np.max(motion_ylims)]\n",
    "\n",
    "    div.spine_settings(\n",
    "        axis_objects=[ax_quiet_th, ax_quiet_sw, ax_quiet_tr, ax_quiet_fm,\n",
    "                      ax_motion_th, ax_motion_sw, ax_motion_tr, ax_motion_fm,\n",
    "                      ax_loc_th, ax_loc_sw, ax_loc_tr, ax_loc_fm,\n",
    "                      ax_disc_th, ax_disc_sw, ax_disc_tr, ax_disc_fm,\n",
    "                      ax_emg_sess, ax_tread_sess, ax_facem_sess, ax_state_sess, ax_trial_count],\n",
    "        settings_dicts=[{**general_settings, \"left\": {**general_settings[\"left\"], \"lim\": theta_ylim, \"ticks\": (\"margin\", 0), \"label\": \"6-10 Hz\\nLFP [mV]\", \"label_pad\": -8}},\n",
    "                        {**general_settings, \"left\": {**general_settings[\"left\"], \"lim\": delta_ylim, \"ticks\": (\"margin\", 0), \"label\": \"0.5-4 Hz\\nLFP [mV]\", \"label_pad\": -8}},\n",
    "                        {**general_settings, \"left\": {**general_settings[\"left\"], \"lim\": tread_ylim, \"ticks\": (\"margin\", 0), \"label\": \"Velocity\\n[cm/s]\", \"label_pad\": -5}},\n",
    "                        {**general_settings, \"left\": {**general_settings[\"left\"], \"lim\": motion_ylim, \"ticks\": (\"data_lim\", 0), \"label\": \"Norm. face\\nmotion [A.U.]\"},\n",
    "                                             \"bottom\": {**general_settings[\"bottom\"], \"ticks\": ([0, 2], 0), \"label\": \"Time [sec]\", \"label_pad\": -10}},\n",
    "                        {**general_settings, \"left\": {**general_settings[\"left\"], \"lim\": theta_ylim}},\n",
    "                        {**general_settings, \"left\": {**general_settings[\"left\"], \"lim\": delta_ylim}},\n",
    "                        {**general_settings, \"left\": {**general_settings[\"left\"], \"lim\": tread_ylim}},\n",
    "                        {**general_settings, \"left\": {**general_settings[\"left\"], \"lim\": motion_ylim},\n",
    "                                             \"bottom\": {**general_settings[\"bottom\"], \"ticks\": ([0, 2], 0), \"label\": \"Time [sec]\", \"label_pad\": -10}},\n",
    "                        {**general_settings, \"left\": {**general_settings[\"left\"], \"lim\": theta_ylim}},\n",
    "                        {**general_settings, \"left\": {**general_settings[\"left\"], \"lim\": delta_ylim}},\n",
    "                        {**general_settings, \"left\": {**general_settings[\"left\"], \"lim\": tread_ylim}},\n",
    "                        {**general_settings, \"left\": {**general_settings[\"left\"], \"lim\": motion_ylim},\n",
    "                                             \"bottom\": {**general_settings[\"bottom\"], \"ticks\": ([0, 2], 0), \"label\": \"Time [sec]\", \"label_pad\": -10}},\n",
    "                        {**general_settings, \"left\": {**general_settings[\"left\"], \"lim\": theta_ylim}},\n",
    "                        {**general_settings, \"left\": {**general_settings[\"left\"], \"lim\": delta_ylim}},\n",
    "                        {**general_settings, \"left\": {**general_settings[\"left\"], \"lim\": tread_ylim}},\n",
    "                        {**general_settings, \"left\": {**general_settings[\"left\"], \"lim\": motion_ylim},\n",
    "                                             \"bottom\": {**general_settings[\"bottom\"], \"ticks\": ([0, 2], 0), \"label\": \"Time [sec]\", \"label_pad\": -10}},\n",
    "                        {**whole_sess_settings, \"left\": {**whole_sess_settings[\"left\"], \"lim\": [0, np.percentile(data_emg, 99)]}},\n",
    "                        {**whole_sess_settings, \"left\": {**whole_sess_settings[\"left\"], \"lim\": [0, np.percentile(treadmill, 99)]}},\n",
    "                        {**whole_sess_settings, \"left\": {**whole_sess_settings[\"left\"], \"lim\": [0, np.percentile(face_motion, 99)]}},\n",
    "                        whole_sess_settings, trial_settings])\n",
    "\n",
    "    if save_fig:\n",
    "        fig_name = \"_\".join([os.path.basename(session), f\"loc_{locomotion_threshold}_{locomotion_time_threshold}_emg_{emg_perc}__fm_{face_motion_perc}_{face_motion_time_threshold}_test1\".replace(\".\",\"\")])\n",
    "        fig.savefig(os.path.join(folder_to_save, fig_name), dpi=300, bbox_inches='tight')\n",
    "        \n",
    "    plt.close(fig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab-debug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
